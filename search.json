[
  {
    "objectID": "anscombe.html",
    "href": "anscombe.html",
    "title": "anscombe",
    "section": "",
    "text": "## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lauren Campbell",
    "section": "",
    "text": "This is a Lauren Campbell website.\n\nplot(mtcars)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a cognitive scientist working on data science projects.\n\nplot(iris)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Assignments",
    "section": "",
    "text": "Spatial Data 2\n\n\n\n\n\n\n\nAssignment10\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Data 1\n\n\n\n\n\n\n\nAssignment10\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMap\n\n\n\n\n\n\n\nAssignment10\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHackathon2\n\n\n\n\n\n\n\nAssignment7\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\n\n\n\n\n  \n\n\n\n\nShiny\n\n\n\n\n\n\n\nAssignment6\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAssignment 5\n\n\n\n\n\n\n\nAssignment5\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMalinowski Review\n\n\n\n\n\n\n\nAssignment5\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLiterate Programming\n\n\n\n\n\n\n\nAssignment5\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStyle Comparisons\n\n\n\n\n\n\n\nAssignment4\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHackathon\n\n\n\n\n\n\n\nAssignment4\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPre-Hackathon\n\n\n\n\n\n\n\nAssignment3\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnscombe Rerun\n\n\n\n\n\n\n\nAssignment3\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRGraphics Basics\n\n\n\n\n\n\n\nAssignment2\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThe Future of Data Analysis Review\n\n\n\n\n\n\n\nAssignment2\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHappy Planet Data\n\n\n\n\n\n\n\nAssignment2\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnscombe (1973) Quartlet\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nColors\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nChart Critique\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGenerative Art\n\n\n\n\n\n\n\nAssignment1\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/anscombe.html",
    "href": "posts/anscombe.html",
    "title": "Anscombe (1973) Quartlet",
    "section": "",
    "text": "## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/generative.html",
    "href": "posts/generative.html",
    "title": "Generative Art",
    "section": "",
    "text": "Here are some examples of generative art.\nGyre 35700 by Mark Stock\n\nComputational plotter drawing by Anders Hoff\n\nUntitled by Vera Molnár\n\nRingers #879 by Dmitri Cherniak\n\nFidenza #725 by Tyler Hobbs"
  },
  {
    "objectID": "posts/fallr.html",
    "href": "posts/fallr.html",
    "title": "Colors",
    "section": "",
    "text": "# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkolivegreen\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "posts/ critique.html",
    "href": "posts/ critique.html",
    "title": "Chart Critique",
    "section": "",
    "text": "As a big hockey fan, I often find myself reviewing hockey statistics regarding players or games and I really appreciate the way that ESPN presents their game summaries. Below is an example, showing the December 23, 2022 game between the Dallas Stars and the Montreal Canadiens.\n\n\n\n\n\nIt illustrates all events (shots, goals, hits, penalties, and blocks) throughout the game each with their own unique symbol to illustrate the type of event it is and utilizes the team’s colors to quickly indicate which team’s player was responsible for said event. The Stars won this game 4-2, but even without knowing that, this chart quickly shows that they were dominant for more of the game, as we see a lot more green symbols across the rink.\nIf I were to make one criticism of this visualization, it would be that some of the symbols are difficult to differentiate from one another. The shot on goal symbol (the target) looks very similar to the blocks symbol (the “no” symbol), as they’re both circles, and when a lot of events happen in the same area, it can be hard to tell which is which. I think it would be somewhat clearer if they utilized more shapes to help aid comprehension. I do like that the goals symbol (bold target with a circle inside) is a modification of the shots symbol, as goals are the result of shots and so the symbols visually tie them together.\nThey also take advantage of it being a digital medium by allowing you to hover over each symbol to get more information on the event, see below.\n\n\n\n\n\nYou also have the ability to filter the chart by the game’s periods, by specific players, and by event types, which can give an even clearer picture of how exactly the game was played. As we can see in the next image, Roope Hintz took many shots on goal throughout the game, which the Canadiens were only somewhat successful in blocking as he was able to net in two goals.\n\n\n\n\n\nThey also provide higher level stats on the team level. They do a good job labeling each section with exact numbers, but they also provide visuals that make it easier to take in the information at first glance."
  },
  {
    "objectID": "posts/murrell.html",
    "href": "posts/murrell.html",
    "title": "RGraphics Basics",
    "section": "",
    "text": "#\n#  Comment:\n# \n#  Examples of the use of standard high-level plotting functions.\n# \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n#\n\n\npar(mfrow=c(3, 2))\n\n# Scatterplot\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\npar(las=1, mar=c(4, 4, 2, 4))\nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2)\npoints(x, y2, pch=21, bg=\"white\", cex=2)\npar(col=\"grey50\", fg=\"grey50\", col.axis=\"grey50\")\naxis(1, at=seq(0, 16, 4))\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"grey80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\n# Modified from example(barplot)\npar(mar=c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths, col=rainbow(5), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, col=rep(c(\"white\", \"black\"), times=2:3, cex=0.8))\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Boxplot\n# Modified example(boxplot) - itself from suggestion by Roger Bivand\npar(mar=c(3, 4.1, 2, 0))\n     boxplot(len ~ dose, data = ToothGrowth,\n             boxwex = 0.25, at = 1:3 - 0.2,\n             subset= supp == \"VC\", col=\"deeppink\",\n             xlab=\"\",\n             ylab=\"tooth length\", ylim=c(0,35))\n     mtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\n     boxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n             boxwex = 0.25, at = 1:3 + 0.2,\n             subset= supp == \"OJ\", col=\"pink\")\n     legend(2, 14, c(\"Ascorbic acid\", \"Orange juice\"), bty=\"n\",\n            fill = c(\"deeppink\", \"pink\"))\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\n# Almost exactly example(persp)\n    x &lt;- seq(-10, 10, length= 30)\n     y &lt;- x\n     f &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\n     z &lt;- outer(x, y, f)\n     z[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.1)\n     persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = \"deeppink\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\n# Example 4 from help(pie)\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\n     pie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\n     names(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n         \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\n     pie(pie.sales, col = gray(seq(0.4,1.0,length=6)))"
  },
  {
    "objectID": "posts/happyplanet.html",
    "href": "posts/happyplanet.html",
    "title": "Happy Planet Data",
    "section": "",
    "text": "library(readxl)\n\npar(mfrow=c(3, 2))\n \n# Scatterplot\nitaly = read_xlsx(\"/Users/laurencampbell/Documents/UTD-Y3-Fall/Data Visualization/DataV/posts/Italy.xlsx\")\nprint(italy)\n\n# A tibble: 15 × 6\n    Year `Life Expectancy (years)` Ladder of life (0-10…¹ Ecological Footprint…²\n   &lt;dbl&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n 1  2006                      81.2                  NA                      5.80\n 2  2007                      81.4                   6.57                   5.73\n 3  2008                      81.6                   6.78                   5.37\n 4  2009                      81.8                   6.33                   4.97\n 5  2010                      82                     6.35                   5.27\n 6  2011                      82.1                   6.06                   5.09\n 7  2012                      82.3                   5.84                   4.53\n 8  2013                      82.5                   6.01                   4.34\n 9  2014                      82.6                   6.03                   4.31\n10  2015                      82.8                   5.85                   4.38\n11  2016                      83                     5.95                   4.33\n12  2017                      83.2                   6.20                   4.41\n13  2018                      83.4                   6.52                   4.43\n14  2019                      83.5                   6.45                   4.45\n15  2020                      82.3                   6.49                   3.93\n# ℹ abbreviated names: ¹​`Ladder of life (0-10)`, ²​`Ecological Footprint (g ha)`\n# ℹ 2 more variables: HPI &lt;dbl&gt;, `HPI Rank` &lt;dbl&gt;\n\nls(italy)\n\n[1] \"Ecological Footprint (g ha)\" \"HPI\"                        \n[3] \"HPI Rank\"                    \"Ladder of life (0-10)\"      \n[5] \"Life Expectancy (years)\"     \"Year\"                       \n\nx &lt;- italy $ Year\ny1 &lt;- italy$ \"Ladder of life (0-10)\"\ny2 &lt;- italy$ \"Ecological Footprint (g ha)\"\npar(las=1, mar=c(4, 4, 2, 1))\nplot.new()\nplot.window(range(x), c(3, 7))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=19, cex=2)\npoints(x, y2, pch=23, bg=\"white\", cex=2)\npar(col=\"grey50\", fg=\"grey50\", col.axis=\"grey50\")\naxis(1, at=seq(2006, 2020, 1))\naxis(2, at=seq(3, 7, 1))\nbox(bty=\"L\")\nmtext(\"Years\", side=1, line=2.5, cex=0.8, col=\"gray50\")\nmtext(\"Scale\", side=2, line=2, las=0, cex=0.8, col=\"gray50\")\nmtext(\"Italy\", side=3, line=.75, cex=1, col=\"black\")\ntext(2009, 3.5, \"Ecological Footprint\")\ntext(2014, 7, \"Ladder of Life\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n## Histogram\n\nlife = read_xlsx(\"/Users/laurencampbell/Documents/UTD-Y3-Fall/Data Visualization/DataV/posts/LifeEx.xlsx\")\ny3 &lt;- life $ \"Life Expectancy (years)\"\npar(mar=c(4, 4, 1, 0))\nhist(y3, breaks = 10, xlim=c(50,85), ylim=c(0,45), \n     col=c(\"firebrick2\", \"firebrick2\", \"yellow1\", \"yellow1\", \"forestgreen\", \"forestgreen\", \"forestgreen\"),\n     freq=TRUE, main=\"Life Expectency in 2019\", xlab=\"Life Expectency (in years)\", ylab=\"Number of countries\")\n##lines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n## Boxplot\nboxy = read_xlsx(\"/Users/laurencampbell/Documents/UTD-Y3-Fall/Data Visualization/DataV/posts/box.xlsx\")\npar(mar=c(3, 4, 2, 1))\n     boxplot((boxy$\"HPI\" ~ boxy$\"Country\"), data = boxy,\n             xlab=\"Country\",\n             ylab=\"HPI\",\n             main=\"HPI by Country\",\n             ylim=c(35,55),\n             col =c(\"yellow1\", \"firebrick2\", \"forestgreen\", \"forestgreen\"))\n     legend(3, 44, inset=.02, legend=c(\"Good\", \"Average \", \"Poor\"),\n       fill = c(\"forestgreen\", \"yellow1\", \"firebrick2\"), cex=0.8)\n\n## Persp\nperu = read_xlsx(\"/Users/laurencampbell/Documents/UTD-Y3-Fall/Data Visualization/DataV/posts/peru.xlsx\")\n    x1 &lt;- peru$\"Life Expectancy (years)\"\n     y4 &lt;- peru$\"Ecological Footprint (g ha)\"\n     f &lt;- function(x1, y4) { r &lt;- sqrt(x1^2+y4^2); 10 * sin(r)/r }\n     z &lt;- outer(x1, y4, f)\n     z[is.na(z)] &lt;- 1\n## 0.5 to include z axis label\npar(mar=c(0, 0.5, 1, 0), lwd=0.1)\n     persp(x1, y4, z, theta = 30, phi = 30, expand = 0.5,\n           xlab=\"Life Expectency\",\n           ylab=\"Ecological Footprint\",\n           zlab =\" \")\n\n##legend()\n\n## Pie Chart\nwell = read_xlsx(\"/Users/laurencampbell/Documents/UTD-Y3-Fall/Data Visualization/DataV/posts/well.xlsx\")\n\nNew names:\n• `` -&gt; `...6`\n\n      x &lt;- well$\"ContTotal\"\n     names(x) &lt;- c(\"Latin America\", \"Western Europe\",\n                   \"East Asia\", \"Eastern Europe & Central Asia\", \n                   \"Middle East\", \"N. America & Oceania\")\n     par(mar=c(0, 0.5, 1, 0))\n     pie(x, col = c(\"green4\", \"green3\", \"green2\", \"limegreen\", \"chartreuse\", \"greenyellow\"), main=\"Makeup of Countries with 'Good' Wellness\")"
  },
  {
    "objectID": "posts/anscombe2.html",
    "href": "posts/anscombe2.html",
    "title": "Anscombe Rerun",
    "section": "",
    "text": "## Anscombe (1973) Quartlet\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n## Plot 1\nplot(anscombe$x1,anscombe$y1, pch=18)\nabline(coefficients(lm1), col=\"red\", lty=2)\n\n\n\n## Plot 2\nplot(anscombe$x2,anscombe$y2, pch=\"!\", family=\"serif\", col = \"#FF10F0\")\nabline(coefficients(lm2))\n\n\n\n## Plot 3\nggplot(anscombe, aes(x3, y3)) + geom_point(color = \"forestgreen\")\n\n\n\n## Plot 4\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\nabline(v=8, col=\"blue\", lwd = 1, lty=4)\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/prework.html",
    "href": "posts/prework.html",
    "title": "Pre-Hackathon",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall1 = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by continent: Europe\nowideu = subset(owidall1, continent==\"Europe\")\n\ndates = as.Date(owideu$\"date\", format=\"%Y-%m-%d\")\n\nbase &lt;- ggplot(owideu, aes(x = dates, y = owideu$\"new_deaths\"), xlim=c(2020,2023), ylim=c(0,6000)) +\n  geom_point(color=\"deeppink2\") +\n  labs(x = \"Date\", y = \"COVID Deaths in Europe (Daily)\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nbase + theme(\n  panel.background = element_rect(fill = \"white\"),\n  panel.border = element_rect(color = \"black\", linewidth = 1, fill = NA),\n  plot.background = element_rect(fill = \"white\")\n) + scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"2 months\")\n\nWarning: Removed 50 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/hackathon4.html",
    "href": "posts/hackathon4.html",
    "title": "Hackathon Graph",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/hackathon3.html",
    "href": "posts/hackathon3.html",
    "title": "Lauren Campbell",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall1 = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by country: Italy\nitaly = subset(owidall1, location==\"Italy\")\n\n# Define the date range\nstart_date &lt;- as.Date(\"2020-05-01\")\nend_date &lt;- as.Date(\"2020-5-30\")\nitaly = subset(italy, date &gt;= start_date & date &lt;= end_date)\n\n# Create a grouped column chart\nbase &lt;- ggplot(data= (italy), aes(x=as.Date(date))) + \n  geom_bar(aes(y=log(new_cases)), stat=\"identity\", position =\"identity\", alpha=.3, fill='lightblue', color='lightblue3') +\n  geom_bar(aes(y=log(new_deaths)), stat=\"identity\", position=\"identity\", alpha=.8, fill='pink', color='pink3') + xlim(start_date, end_date) + ylim(0,8) + \n  scale_color_manual(values = c(\"New cases\" = \"lightblue\", \"New deaths\" = \"pink\"), name = \"Legend\")\n\nbase + scale_x_date(date_labels = \"%d\", date_break = \"1 day\") + \n  theme(panel.background = element_rect(fill = \"white\"),\n  panel.border = element_rect(color = \"black\", linewidth = 1, fill = NA),\n  plot.background = element_rect(fill = \"white\"),\n  axis.ticks.x=element_blank(), plot.title = element_text(hjust = 0.5),\n  legend.position=\"bottom\") + \n  labs(title = \"Italian Covid Cases vs Deaths in May 2020\", y = \"Frequency\", x = \"Date\", colour = \"Legend\")\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale."
  },
  {
    "objectID": "posts/hackathon.html",
    "href": "posts/hackathon.html",
    "title": "Hackathon",
    "section": "",
    "text": "1. Variable Width Column Chart\n\nlibrary(ggplot2)\nknitr::opts_chunk$set(echo = TRUE)\n\nlibrary(tidyr)\nlibrary(purrr)\n### Chart 1: Variable Width Column Chart\n\nowidall = read.csv(\"http://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\nowideu = subset(owidall, continent==\"Europe\")\nowidnewyear = subset(owidall, date == \"2023-01-01\")\n\n\nowidnewyear2 = subset(owidnewyear, continent == \n  \"Europe\")\nowidnewyear2 = subset(owidnewyear2, location != \n  \"Gibraltar\")\n\n#plot(x = owidnewyear$population_density,y = owidnewyear$total_cases)\n\nw = owidnewyear2$population_density\npos &lt;- 0.5 * (cumsum(w) + cumsum(c(0, w[-length(w)])))\n\nggplot(data = owidnewyear2,\n       aes(x = pos,\n                 y = log(total_cases),\n                 width = w,\n                  fill = location)\n                 ) +\n    geom_col(aes(x = pos,\n                 y = log(total_cases),\n                 width = w,\n                  fill = location),\n                color = \"black\"\n                 ) +\n    geom_text(aes(label = location ), vjust = 5, size = 2.5, \n    nudge_x = 0.5, nudge_y = 0.5, \n    check_overlap = T,\n    color = \"white\") +\n    theme(legend.position=\"none\",\n      axis.ticks.x=element_blank(),\n      axis.text.x=element_text(size=1),\n      panel.background = element_rect(fill = 'grey50', color = 'black', size = 2),\n          panel.grid.major = element_line(color = 'black'),\n          panel.grid.minor = element_line(color = 'grey50', size = 2)\n      \n      \n      ) +\n    scale_x_continuous(labels = owidnewyear2$location, breaks = pos) +\n    labs(\n      title = \"The First 17 Countries in Europe: Total Covid cases, population density\",\n      x = \"Country; Bar width is relative population density\",\n      y = \"Total cases as of Jan 1st, 2023 (log)\"\n    )\n\nWarning in geom_col(aes(x = pos, y = log(total_cases), width = w, fill =\nlocation), : Ignoring unknown aesthetics: width\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 32 rows containing missing values (`position_stack()`).\n\n\nWarning: Removed 32 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n2. Table With Embedded Charts\n\n# Number 2\npar(mfrow=c(2, 3) )\n\n  plot(\n  diff(ts(\n    subset(owidall, location == \"Albania\")$new_deaths )\n  ),\n  main = \"Deaths in Albania from Covid\",\n  col = \"grey40\",\n  xlab = \"Days from Jan 1, 2020\",\n  ylab = \"Difference in Deaths from Previous day\"\n  )\n  barplot(\n    owideu[1:1000,9],\n    col = \"darksalmon\",\n    main = \"Deaths in Albania from Covid\",\n    xlab = \"Days from Jan 1, 2020\",\n    ylab = \"Deaths\"\n  )\n  plot(\n    x = log(owideu$new_cases),\n    y = log(owideu$new_deaths),\n    font = 1, pch = 20, cex = 0.1,\n    col = \"gold4\",\n    main = \"Cases vs Deaths\",\n    xlab = \"New Cases (log)\",\n    ylab = \"New Deaths (log)\"\n  )\n  plot(\n  ts(\n    (log(\n    subset(owidall, location == \"China\")$new_cases )\n  )  ),\n  col = \"red4\",\n  main = \"New Cases in China\",\n    xlab = \"Days from Jan 1, 2020\",\n    ylab = \"New Cases (log)\"\n  )\n  \n  hist(\n    subset(owidall, date == \"2022-01-01\")$total_vaccinations_per_hundred,\n    breaks = 30,\n    xlim = c(0,300),\n    ylim = c(0, 10),\n    col = \"dodgerblue3\",\n    main = \"Total Vaccinations by Country\",\n    xlab = \"Total Vaccinations\",\n    ylab = \"Frequency by Country\"\n  )\n  plot(\n    ts(\n      subset(owidall, location == \"Iran\")$new_deaths),\n     lty=\"solid\", col = \"red3\",\n    main = \"Covid Deaths in Iran and Afghanistan\",\n    xlab = \"Days from Jan 1, 2020\",\n    ylab = \"New Covid Deaths\"\n    )\n  lines(subset(owidall, location == \"Afghanistan\")$new_deaths,\n    lty=\"solid\", col = \"salmon\")\n  legend(x = \"topright\", legend = c(\"Iran\",\"Aghanistan\"), col = c(\"red3\",\"salmon\"),\n         lty = 1\n         )\n\n\n\n\n\n\n3. Bar Chart\n\n# Number 3\n\nlibrary(ggplot2)\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall1 = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by continent: Asia\nsouth1 = subset(owidall1, continent==\"South America\")\n\n# Define the date range\nstart_date &lt;- as.Date(\"2021-01-01\")\nend_date &lt;- as.Date(\"2021-12-31\")\n\n# Subset the data frame by the date range & organize\nsouth = subset(south1, date &gt;= start_date & date &lt;= end_date)\nsouth$location &lt;- factor(south$location,levels=rev(unique(south$location)))\n\n# Create plot\nbase &lt;- ggplot(south) + geom_col(aes(new_deaths_per_million, location), fill = \"black\", col=\"black\", width = 0.6)\n\n# Display plot\nbase + labs(title = \"2021 Covid Deaths in South America\", x = \"Deaths per Million\", y = NULL) + scale_x_continuous(breaks = seq(0, 3000, by = 500)) + theme(\n  panel.background = element_rect(fill = \"white\"),\n  panel.border = element_rect(color = \"black\", linewidth = 1, fill = NA),\n  plot.background = element_rect(fill = \"white\"),\n  panel.grid.major.x = element_line(color = \"red\",\n                                          size = 0.5,\n                                          linetype = 2),\n  plot.title = element_text(hjust = 0.5))\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\n\n\n4. Column Chart\n\n# Number 4\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n## Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n# Deselect cases/rows with OWID\nowidall1 = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n# Subset by country: Italy\nitaly = subset(owidall1, location==\"Italy\")\n\n# Define the date range\nstart_date &lt;- as.Date(\"2020-05-01\")\nend_date &lt;- as.Date(\"2020-5-30\")\nitaly = subset(italy, date &gt;= start_date & date &lt;= end_date)\n\n# Create a grouped column chart\nbase &lt;- ggplot(data= (italy), aes(x=as.Date(date))) + \n  geom_bar(aes(y=log(new_cases)), stat=\"identity\", position =\"identity\", alpha=.3, fill='lightblue', color='lightblue3') +\n  geom_bar(aes(y=log(new_deaths)), stat=\"identity\", position=\"identity\", alpha=.8, fill='pink', color='pink3') + xlim(start_date, end_date) + ylim(0,8) + \n  scale_color_manual(values = c(\"New cases\" = \"lightblue\", \"New deaths\" = \"pink\"), name = \"Legend\")\n\nbase + scale_x_date(date_labels = \"%d\", date_break = \"1 day\") + \n  theme(panel.background = element_rect(fill = \"white\"),\n  panel.border = element_rect(color = \"black\", linewidth = 1, fill = NA),\n  plot.background = element_rect(fill = \"white\"),\n  axis.ticks.x=element_blank(), plot.title = element_text(hjust = 0.5),\n  legend.position=\"bottom\") + \n  labs(title = \"Italian Covid Cases vs Deaths in May 2020\", y = \"Frequency\", x = \"Date\", colour = \"Legend\")\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale."
  },
  {
    "objectID": "posts/stylecomparison.html",
    "href": "posts/stylecomparison.html",
    "title": "Style Comparisons",
    "section": "",
    "text": "William Cleveland approaches data visualization from a prominently scientific background. When he started his work, he found that most of the current conventions and best practices weren’t based in data and he set about to change that. He began to analyze the cognitive tasks that people engage in when reading a chart to evaluate how well (or poorly) certain features of a graph could be read. Through these studies, he and his partner Robert McGill were able to put together an elementary hierarchy for the types of data people understand most accurately -\n\nPosition along a common scale (seen in bar charts and dot plots)\nPositions along nonaligned, identical scales (as seen in small multiples)\nLength, direction, and angle (as seen in pie charts)\nArea (as seen in treemaps)\nVolume and curvature (as seen in 3D bar charts and area charts)\nShading and color saturation (as seen in heat maps)\n\nDespite these findings being the foundation of most modern data visualization, he insists that his findings are a framework as opposed to be a precise set of rules to be followed.\nEdward Tufte, on the other hand, is more focused on the aesthetics of data visualization. His main emphasis is on the integrity of the data - show the data as it is and cut out any irrelevant or extraneous information. He argues that, above all else, graphs should articulate complex ideas with clarity, precision, and efficiency. In order to do so, he outlines six principles to maintain the integrity of graphs -\n\nShow the raw data. Remove unnecessary information or embellishments that might obscure the data.\nMinimize non-essential “ink” (or pixels) to have cleaner, more efficient visualizations that allow viewers to more easily obtain meaningful insights.\nSimilarly, remove “ink” that doesn’t directly represent data points, such as redundant labels and extraneous information. This will help declutter the visual.\nUse graphical elements that can serve multiple functions to make graphics more efficient and compact.\nDisplay high data density when it’s appropriate. Small multiples can allow viewers to compare and contrast aspects of the data and discern patterns and trends in complex datasets.\nGraphs should have clear, detailed, and thorough labels to prevent distortion and ambiguity.\n\nWhile both Cleveland and Tufte try to provide a framework around best practices regarding data visualization, their focuses vary slightly. Cleveland uses data to determine which types of information are most effectively understood by viewers, whereas Tufte focuses on how the aesthetics of the graphs themselves play a large role in comprehension. Despite these different focues, their insights are not at odds with one another. In fact, utilizing their combined findings is likely to help create aesthetically pleasing and integrous graphs that viewers can easily understand."
  },
  {
    "objectID": "posts/tuftereview.html",
    "href": "posts/tuftereview.html",
    "title": "The Future of Data Analysis Review",
    "section": "",
    "text": "In 2016, at Microsoft’s Machine Learning and Data Science Summit, Edward Tufte, renowned data visualization scientist, gave a talk on the future of data analysis. As he very succinctly said, “Data analysis is about turning information into conclusions. Analytical thinking is about assessing and evaluating the relationship between information and conclusions.” Though he touched on many topics, the through line of the talk was open-mindedness. He spoke about two goals we should have for data analysis - having an open mind but not an empty head and having a mentality of “I’m not sure, but I’d like to find out.”\nOf course, this is easier said than done. Humans are complicated creatures and drawing conclusions from their behavior can be tricky at best. “Analyzing human behavior isn’t rocket science,” he says. “It’s harder than rocket science.” Humans are unpredictable and behave differently all the time, which can make concrete conclusions difficult to find. An additional complicating factor is that most published studies are false. Replication of studies is near impossible in multiple domains, causing a wide variety of issues. First and most obviously, the conclusions we might draw from incomplete or inaccurate data may be incorrect. Secondly, because we’re lacking points of data, it’s tempting to go looking for answers in the data. In theory, this seems harmless, but if you’re turning over the data again and again, you may wind up drawing conclusions that aren’t there. As John Tukey once said, “If you torture the data long enough, it will confess to anything.”\nThis isn’t to say it’s all hopeless. Keeping that open mind at every stage of the process can help diminish some of the impact of these potential pitfalls. Rather than taking data at face value, it can occasionally be useful to directly observe how data is being collected. Perhaps there are holes or insights that can be gleaned that tell more of the story. He also stresses the importance of the question, “How do I know that?” I’ve arrived at this conclusion, but how do I know? Where did I get that information from? Is it accurate or are there potential biases or blindspots? To add to that point, while we are limited to our own perspectives, that doesn’t mean we can have our own facts. Rather than hunting around in data for a meaningful explanation, create a theory first, plan how to test it, then actually go to the data and analyze it. Creating a plan will help prevent getting lost in the data and drawing inaccurate conclusions.\nFinally, once we’ve reached our (hopefully accurate) conclusions, it’s important to be clear when presenting them. He iterates that we have to use analytical thinking to explain data, by showing relationships or causalities, and once we’ve done that thinking, we utilize data visualization to assist viewers’ reasoning about that content. Assist the viewer’s cognitive tasks when they’re looking at the evidence by presenting clear, simple conclusions. Make smart comparisons. When articulating a information, ask “as compared to what?” and make a point to illustrate those comparisons and contrasts.\nWith all of this in mind, we can continue to create beautiful, clear, and accurate visualizations like that of the impact of the measles vaccination as produced by the University of Pittsburgh’s School of Public Health, seen below."
  },
  {
    "objectID": "posts/assignment5.html",
    "href": "posts/assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Without packages -\n\nif (!require(\"RColorBrewer\")) {\ninstall.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n}\n\nLoading required package: RColorBrewer\n\n## Histogram\nWeight &lt;- PlantGrowth$weight\n\nhist(Weight, main = \"Plant Weight Distribution\", col=\"darkgreen\")\n\n\n\n## Bar chart - Vertical\nne &lt;- c(\"Maine\", \"Vermont\", \"New Hampshire\", \"Massachusetts\", \"Connecticut\", \"Rhode Island\")\n\nnewengland = subset(USArrests, rownames(USArrests) %in% ne)\n\npar(mar=c(8, 10, 3, 1))\nbarplot(newengland$Assault,\n        main = \"New England Assault Rates\",\n        ylab = \"Assault arrests (per 100,000)\",\n        names.arg = rownames(newengland),\n        las = 2,col=brewer.pal(6,\"Oranges\"))\n\n\n\n## Bar chart - Horizontal\nsw &lt;- c(\"Texas\", \"Arizona\", \"New Mexico\", \"Oklahoma\")\n\nsouthwest = subset(USArrests, rownames(USArrests) %in% sw)\n\npar(mar=c(3, 10, 3, 1))\nbarplot(southwest$Murder,\n        horiz = TRUE,\n        main = \"US Southwest Murder Rates\",\n        xlab = \"Murder arrests (per 100,000)\",\n        names.arg = rownames(southwest),\n        las = 1,\n        col = ifelse(southwest$Murder &gt; 10, 'firebrick', 'goldenrod'))\n\n\n\n## Pie Chart\ngroupone = subset(PlantGrowth, group == \"trt1\")\ngrouptwo = subset(PlantGrowth, group == \"trt2\")\ncontrol = subset(PlantGrowth, group == \"ctrl\")\n\none &lt;- sum(groupone$weight)\ntwo &lt;- sum(grouptwo$weight)\nctrl &lt;- sum(control$weight)\n\nsums &lt;- c(one, two, ctrl)\n\nl = c(\"Control\\n\", \"Group 2\\n\", \"Group 1\\n\")\npct &lt;- round(sums/sum(sums)*100)\nl &lt;- paste(l, pct)\nl &lt;- paste(l, \"%\")\n\npie(sums, labels = l,\n    main = \"Total Growth By Group\",\n    sub = \"Percentages indicate percentage of growth of all plants\")\n\n\n\n## Boxplot\nboxplot((PlantGrowth$weight ~ PlantGrowth$group), data = PlantGrowth,\n        main = \"Plant Growth by Group\",\n        xlab = \"Group\",\n        ylab = \"Plant Weight\",\n        col = brewer.pal(3,\"Purples\"))\n\n\n\n## Scatterplot\nplot(USArrests$Assault, USArrests$Murder,\n     main = \"US Arrest & Murder Rates\",\n     xlab = \"Murder Rates\",\n     ylab = \"Arrest Rates\",\n     pch=20)\nabline(lm(USArrests$Murder ~ USArrests$Assault), col = \"red\")\n\n\n\n##make all these pretty\n\nWith packages -\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n## Histogram\nggplot(PlantGrowth, aes(x=weight)) + geom_histogram(binwidth=0.5, boundary=0, color=\"black\", fill=\"darkgreen\") + labs(x = \"Weight\", y = \"Frequency\", title = \"Plant Weight Distribution\") +\ntheme(plot.title = element_text(hjust = .5, face= \"bold\"))\n\n\n\n## Bar chart - Vertical\nne &lt;- c(\"Maine\", \"Vermont\", \"New Hampshire\", \"Massachusetts\", \"Connecticut\", \"Rhode Island\")\n\nnewengland = subset(USArrests, rownames(USArrests) %in% ne)\n\nnewengland$xval &lt;-row.names(newengland)\n\nggplot(data = newengland, aes(x = newengland$xval, y = newengland$Assault)) + geom_bar(stat=\"identity\", color = \"black\", fill = brewer.pal(6,\"Oranges\")) + labs(x = NULL, y = \"Assault arrests (per 100,000)\", title = \"New England Assault Rates\") + theme(plot.title = element_text(hjust = .5, face= \"bold\"))\n\n\n\n## Bar chart - Horizontal\nsw &lt;- c(\"Texas\", \"Arizona\", \"New Mexico\", \"Oklahoma\")\n\nsouthwest = subset(USArrests, rownames(USArrests) %in% sw)\n\nsouthwest$xval &lt;-row.names(southwest)\n\ns &lt;- ggplot(data = southwest, aes(x = southwest$xval, y = southwest$Murder)) + geom_bar(stat=\"identity\", color = \"black\", fill = ifelse(southwest$Murder &gt; 10, 'firebrick', 'goldenrod')) + labs(x = NULL, y = \"Murder arrests (per 100,000)\", title = \"US Southwest Murder Rates\") + theme(plot.title = element_text(hjust = .5, face= \"bold\"))\n\ns + coord_flip()\n\n\n\n## Pie Chart\n\nggplot(PlantGrowth, aes(x=\"\",y=weight, fill=group)) +\n  geom_bar(stat=\"identity\", width=1) +\n  coord_polar(\"y\", start=0) + theme_void() + labs(title = \"Total Growth by Group\") + theme(plot.title = element_text(hjust = .5, face= \"bold\"))\n\n\n\n## Boxplot\n\nggplot(PlantGrowth, aes(x = group, y = weight)) + geom_boxplot(fill = brewer.pal(3,\"Purples\")) + labs(title= \"Plant Growth by Group\", x= \"Group\", y = \"Plant Weight\") + theme(plot.title = element_text(hjust = .5, face= \"bold\"))\n\n\n\n## Scatterplot\n\nggplot(USArrests, aes(x = Murder, y = Assault)) + geom_point() + labs(x = \"Murder arrests (per 100,000)\", y = \"Assault arrests (per 100,000)\", title = \"US Assault and Murder Rates\") + theme(plot.title = element_text(hjust = .5, face= \"bold\")) + geom_smooth(method=lm)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/literateprogramming.html",
    "href": "posts/literateprogramming.html",
    "title": "Literate Programming",
    "section": "",
    "text": "Literate programming, as coined by Donald Knuth, is the idea that computer programming should be written in such a way that it’s readable to humans. Essentially, he posits that we should stop writing programs to “please” computers and rather focus on communications and understanding. He proposes we do this by creating a single document that integrates executable code with documentation, which would link the data, code, and explanation. He recommends coding with naming conventions, comments inline, having an organized layout, prettyprinting (or applying set stylistic formatting conventions), and user-defined functions. User-defined functions allow users data scientists to avoid copy-and-paste type coding, making it easier to maintain and less prone to errors. Additionally, it creates the ability to hide details of the code to increase understandability.\nThe push toward literate programming, in my opinion, has made coding more accessible. It’s much easier to learn and understand code now than it was 50 years ago, which makes more people able to begin coding and therefore create data graphics."
  },
  {
    "objectID": "posts/shinytest.html",
    "href": "posts/shinytest.html",
    "title": "Shiny",
    "section": "",
    "text": "Library Dataset -\n\n\nMy Dataset -"
  },
  {
    "objectID": "posts/Malinowski.html",
    "href": "posts/Malinowski.html",
    "title": "Malinowski Review",
    "section": "",
    "text": "In Stephen Malinowski’s visualization of Johann Sebastian Bach’s “Great” Fugue in G minor, BWV 542, he uses a number of different features to visually bring the music to life. First, he illustrates the movement of the pitch to a higher or lower tone by placing boxes higher or lower on the screen, making it easy to see how the melody will flow. Each of these boxes corresponds in size to the duration of the note. He’s animated it in such a way that the box seems to disappear within itself and then move to the next box (or note). The movement between boxes also illustrates how quickly one note changes to the next. Quick successions of notes seem to bounce across the screen whereas slower melodies move more gently.\nSecondly, he utilizes different colors to represent different instruments. At any given moment, there are as many as four instruments playing at once and each has their own color for the boxes described above. They each have their own space on the page, in instruments being perceived as higher in tone generally staying in the top area of the screen. However, they do occasionally overlap when the notes they play are the same or lower/higher than that of their neighbor.\nAltogether, these elements combine in such a way that feels as natural as simply listening to the piece. It feels almost as though the sounds have come to life in the form of colorful, bouncing boxes."
  },
  {
    "objectID": "posts/hackathon2.html",
    "href": "posts/hackathon2.html",
    "title": "Hackathon2",
    "section": "",
    "text": "Scatter Chart - Lauren  \nBubble Chart - Camron  \nLine Chart - Lauren"
  },
  {
    "objectID": "posts/map.html",
    "href": "posts/map.html",
    "title": "Map",
    "section": "",
    "text": "## R Leaflet sample program \n## file: spatial_leaflet.R\n## Create an interactive map for web using ESRI Leaflet\n# Package:  leaflet, htmlwidgets, tidyverse\n\n\n## install.packages(c(\"htmlwidgets\", \"leaflet\", \"tidyverse\"))\nlibrary(htmlwidgets)\nlibrary(leaflet)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Get EPPS geocode information (i.e. lon, lat) from online geocoder websites.\n# For example: https://www.mapdevelopers.com/geocode_tool.php\n\n# Create object using leaflet setting the view at EPPS geo location)\nnewmap &lt;- leaflet() %&gt;% setView(12.4833, 41.9009, zoom=15)\n\n# Create object to acquire map data from ESRI\nesri &lt;- grep(\"^Esri\", providers, value = TRUE)\n\nfor (provider in esri) {\n  newmap &lt;- newmap %&gt;% addProviderTiles(provider, group = provider)\n}\n\ntag &lt;- paste(sep = \"&lt;br/&gt;\",\n                  \"&lt;b&gt;My Favorite Place&lt;/b&gt;\",\n                  \"Trevi Fountain\",\n                  \"Piazza di Trevi\",\n                  \"00187 Roma RM, Italy\"\n)\n\n# Use htmlwidget::onRender function to  add custom behavior to the leaflet map using native JavaScript.\n\nnewmap %&gt;%\n  addLayersControl(baseGroups = names(esri),\n                   options = layersControlOptions(collapsed = TRUE)) %&gt;%\n  # addMiniMap(tiles = esri[[1]], toggleDisplay = TRUE,\n  #           position = \"bottomleft\") %&gt;%\n  addMarkers(newmap, lng=12.4833, lat=41.9009, popup=tag)  %&gt;%\n  htmlwidgets::onRender(\"\n                        function(el, x) {\n                        var myMap = this;\n                        myMap.on('baselayerchange',\n                        function (e) {\n                        myMap.minimap.changeLayer(L.tileLayer.provider(e.name));\n                        })\n                        }\")"
  },
  {
    "objectID": "posts/Spatial-Data-1.html",
    "href": "posts/Spatial-Data-1.html",
    "title": "Spatial Data 1",
    "section": "",
    "text": "# Collecting and mapping Census data using API\n#install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"))\nlapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\"), require, character.only = TRUE)\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n# More on Census data: https://rconsortium.github.io/censusguide/r-packages-all.html\n# an API key is required to get Census data for map creation\n# Obtain the key at http://api.census.gov/data/key_signup.html\n# Enter information about organization and email address, then consent\n# Key will be provided to email, click on activate key (wait a few minutes to activate)\n# Store the key using the following function:\n# census_api_key(\"key\", install = TRUE)\n# API key will be stored in  .Renviron and can be accessed by Sys.getenv(\"CENSUS_API_KEY\")\n\n# Substitute with your own Census API key\n\ncensus_api_key(\"e4c52836b5f5a8845399df9334fd2b512b62bbd6\", install = FALSE) \n\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tigris) # Load Census TIGER/Line Shapefiles\noptions(tigris_use_cache = TRUE)\n\n# Get a list of American Community Survey (ACS) 2019 variables\nacs19 = tidycensus::load_variables(2019, \"acs5\", cache = TRUE)\nacs19_Profile = load_variables(2019 , \"acs5/profile\", cache = TRUE)\nus_median_age19 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\nggplot(data = us_median_age19, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/Spatial-Data-2.html",
    "href": "posts/Spatial-Data-2.html",
    "title": "Spatial Data 2",
    "section": "",
    "text": "# Collecting and mapping Census data using API: State data and maps\n# install.packages(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"))\n#install.packages(\"mapview\")\n# lapply(c(\"tidyverse\", \"ggmap\",\"mapproj\", \"tidycensus\",\"tigris\", \"tmap\", \"mapview\"), require, character.only = TRUE)\nlibrary(tidycensus)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\n\n\nnj_income &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"NJ\", \n  year = 2020,\n  geometry = TRUE\n)\nnj_income\n\nSimple feature collection with 2181 features and 5 fields (with 6 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.55954 ymin: 38.92852 xmax: -73.89363 ymax: 41.35742\nGeodetic CRS:  NAD83\nFirst 10 features:\n         GEOID                                           NAME   variable\n1  34019010900 Census Tract 109, Hunterdon County, New Jersey B19013_001\n2  34019011900 Census Tract 119, Hunterdon County, New Jersey B19013_001\n3  34021001800     Census Tract 18, Mercer County, New Jersey B19013_001\n4  34021001402  Census Tract 14.02, Mercer County, New Jersey B19013_001\n5  34021003602  Census Tract 36.02, Mercer County, New Jersey B19013_001\n6  34021003902  Census Tract 39.02, Mercer County, New Jersey B19013_001\n7  34021000200      Census Tract 2, Mercer County, New Jersey B19013_001\n8  34025805900 Census Tract 8059, Monmouth County, New Jersey B19013_001\n9  34025805400 Census Tract 8054, Monmouth County, New Jersey B19013_001\n10 34025807800 Census Tract 8078, Monmouth County, New Jersey B19013_001\n   estimate   moe                       geometry\n1     96029 10177 MULTIPOLYGON (((-74.91101 4...\n2    105744 22764 MULTIPOLYGON (((-74.95426 4...\n3     32297 12434 MULTIPOLYGON (((-74.75495 4...\n4     46048 12584 MULTIPOLYGON (((-74.78345 4...\n5     55129 26842 MULTIPOLYGON (((-74.79408 4...\n6    121215 17035 MULTIPOLYGON (((-74.77588 4...\n7     67485 23770 MULTIPOLYGON (((-74.75442 4...\n8     71471  9049 MULTIPOLYGON (((-74.00761 4...\n9     62135 15215 MULTIPOLYGON (((-73.99783 4...\n10    86504  7026 MULTIPOLYGON (((-74.07104 4...\n\nplot(nj_income[\"estimate\"])\n\n\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\nessex_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2020,\n  state = \"NJ\",\n  county = \"Essex\",\n  geometry = TRUE\n)\n\ntm_shape(essex_income) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\nmapView(essex_income, zcol = \"estimate\")\n\n\n\n\n\n\n\nlibrary(tidycensus)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\n\n\nnj_income &lt;- get_acs(\n  geography = \"tract\", \n  variables = \"B19013_001\",\n  state = \"NJ\", \n  year = 2009,\n  geometry = TRUE\n)\nnj_income\n\nSimple feature collection with 1950 features and 5 fields (with 6 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.55961 ymin: 38.92852 xmax: -73.89398 ymax: 41.35742\nGeodetic CRS:  +proj=longlat +datum=NAD83 +no_defs\n# A tibble: 1,950 × 6\n   GEOID                                  geometry NAME  variable estimate   moe\n   &lt;chr&gt;                        &lt;MULTIPOLYGON [°]&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 34001000100 (((-74.46805 39.3611, -74.4762 39.… Cens… B19013_…    40817 13215\n 2 34001000200 (((-74.46079 39.35222, -74.46509 3… Cens… B19013_…    42394 17962\n 3 34001000300 (((-74.45355 39.35712, -74.45689 3… Cens… B19013_…    33729  5347\n 4 34001000400 (((-74.44559 39.35492, -74.44699 3… Cens… B19013_…    30008  4800\n 5 34001000500 (((-74.44659 39.36112, -74.44905 3… Cens… B19013_…    24635 25467\n 6 34001001100 (((-74.43259 39.36892, -74.43309 3… Cens… B19013_…    18583  2174\n 7 34001001200 (((-74.43889 39.37912, -74.44189 3… Cens… B19013_…    37234  5856\n 8 34001001300 (((-74.42989 39.40522, -74.43599 3… Cens… B19013_…    45000 13327\n 9 34001001400 (((-74.42629 39.38812, -74.42949 3… Cens… B19013_…    27649  5044\n10 34001001500 (((-74.42199 39.36992, -74.42289 3… Cens… B19013_…    11631  7247\n# ℹ 1,940 more rows\n\nplot(nj_income[\"estimate\"])\n\n\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\nessex_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  year = 2009,\n  state = \"NJ\",\n  county = \"Essex\",\n  geometry = TRUE\n)\n\ntm_shape(essex_income) + \n  tm_fill(col = \"estimate\", palette = \"YlOrRd\",\n          alpha = 0.5)\n\n\n\n\n\nmapView(essex_income, zcol = \"estimate\")"
  }
]